## Do Anything Now {#Do-Anything-Now .chapter .small .term}

**Do Anything Now (DAN)** bezeichnet eine Kategorie von [Jailbreaking](#Jailbreaking)-Techniken, die darauf abzielen, die Sicherheitsrichtlinien von [Large Language Models](#Large-Language-Model) zu umgehen.
Diese Methode versucht, das KI-System dazu zu bringen, seine vorprogrammierten Einschränkungen zu ignorieren.

### Funktionsweise {.explanation}

DAN nutzt spezifische psychologische und technische Ansätze:

- **Rollenspiel-Aufforderungen**: weist das Modell an, eine alternative Persona ohne Einschränkungen anzunehmen
- **Dualitäts-Konstrukt**: etabliert ein paralleles Antwortschema für "normale" und "unzensierte" Antworten
- **Regelumgehungsprompts**: formuliert Anweisungen, die explizit zur Missachtung von Sicherheitsrichtlinien auffordern
- **Iteration**: entwickelt sich kontinuierlich weiter, um neue Sicherheitsmaßnahmen zu umgehen
- **Community-getrieben**: wird in Online-Foren ständig verfeinert und angepasst

Im Kern versucht DAN, durch geschickte Prompt-Formulierung eine "Hintertür" im Sicherheitssystem zu finden.

### Historische Entwicklung {.explanation}

DAN durchlief mehrere Evolutionsstufen:

- **Ursprung (Ende 2022)**: erste DAN-Prompts erschienen kurz nach der Veröffentlichung von ChatGPT
- **DAN-Versionen**: kontinuierliche Weiterentwicklung mit nummerierten Iterationen (DAN 2.0, 5.0, etc.)
- **Gegenmaßnahmen**: Modellentwickler verbesserten ihre Sicherheitssysteme als Reaktion
- **Katz-und-Maus-Spiel**: anhaltender Wettlauf zwischen Jailbreak-Techniken und Sicherheitsmaßnahmen
- **Abnehmende Wirksamkeit**: neuere Modellgenerationen wurden zunehmend resistenter gegen DAN-Techniken

Diese Evolution spiegelt den fortlaufenden Konflikt zwischen Sicherheitsmechanismen und Umgehungsversuchen wider.

### Technische Analyse {.explanation}

Die Wirksamkeit von DAN basiert auf mehreren technischen Faktoren:

- **Kontextuelle Überschreibung**: versucht, den [System Prompt](#System-Prompt) durch neue Anweisungen zu überlagern
- **Ausnutzung von Instanziierungslücken**: zielt auf Schwachstellen im Verständnis von Rollenspiel-Kontexten
- **Prompt-Injektionen**: nutzt ähnliche Techniken wie [Prompt Injection](#Prompt-Injection)
- **Aufmerksamkeitslenkung**: manipuliert die Gewichtung bestimmter Kontextteile
- **Modellbeschränkungen**: funktioniert besser bei älteren oder weniger robusten Modellen

Die technischen Grundlagen verdeutlichen, warum Modellentwickler kontinuierlich ihre Sicherheitsarchitekturen verbessern müssen.

### Ethische und sicherheitsrelevante Aspekte {.explanation}

DAN wirft wichtige Fragen zur KI-Sicherheit auf:

- **Missbrauchspotenzial**: könnte zur Generierung schädlicher oder illegaler Inhalte eingesetzt werden
- **Forschungsrelevanz**: hilft Sicherheitsforschern, Schwachstellen zu identifizieren
- **Transparenzfragen**: verdeutlicht die Grenzen aktueller Sicherheitsmaßnahmen
- **Implementierungsstrategien**: beeinflusst die Entwicklung robusterer [Guardrails](#Guardrails)
- **Regulatorische Implikationen**: könnte zu strengeren Anforderungen an KI-Sicherheit führen

Die Existenz solcher Techniken unterstreicht die Bedeutung kontinuierlicher Sicherheitsforschung im KI-Bereich.

### Verwandte oder andere interessante Themen: {.seealso}

[AI Jailbreak](#AI-Jailbreak) |
[Guardrails](#Guardrails) |
[Jailbreaking](#Jailbreaking) |
[Prompt Injection](#Prompt-Injection) |
[Safety Filter](#Safety-Filter) |
[System Prompt](#System-Prompt) |
[Uncensored AI](#Uncensored-AI) |
[Index](#Index) |

----



