## Machine Intelligence Research Institute {#Machine-Intelligence-Research-Institute .chapter .small .term}

Das **Machine Intelligence Research Institute (MIRI)** ist eine gemeinnützige Forschungsorganisation, die sich auf die langfristige Sicherheit künstlicher Intelligenz konzentriert.
Es gehört zu den ersten Instituten, die sich mit existenziellen Risiken fortschrittlicher KI-Systeme beschäftigen.

### Geschichte und Entwicklung {.explanation}

MIRI durchlief mehrere Entwicklungsphasen:

- **Gründung (2000)**: ursprünglich als "Singularity Institute for Artificial Intelligence" von Eliezer Yudkowsky gegründet
- **Umbenennung (2013)**: Namensänderung zum heutigen "Machine Intelligence Research Institute"
- **Strategische Neuausrichtung (2016)**: verstärkter Fokus auf mathematische Grundlagenforschung
- **"Neue Aufgabe" (2021)**: Verschiebung zu praxisnäheren Ansätzen für [AI Alignment](#AI-Alignment)

Das Institut etablierte sich als Pionier im Bereich der KI-Sicherheitsforschung, noch bevor dieses Thema breitere akademische Anerkennung fand.

### Forschungsschwerpunkte {.explanation}

MIRI konzentriert sich auf fundamentale Probleme der KI-Sicherheit:

- **Formale Verifizierung**: entwickelt mathematische Methoden zur Sicherstellung von KI-Verhalten
- **Entscheidungstheorie**: erforscht die Grundlagen rationaler Entscheidungsfindung bei KI-Systemen
- **Wertausrichtung**: untersucht Möglichkeiten, menschliche Werte in KI-Systeme zu übertragen
- **[Outer versus Inner Alignment](#Outer-versus-Inner-Alignment)**: analysiert Diskrepanzen zwischen programmierten Zielen und tatsächlichem Verhalten
- **Risiken superintelligenter Systeme**: untersucht potenzielle Gefahren durch [Superintelligence](#Superintelligence)

Diese Forschungsgebiete adressieren Probleme, die besonders relevant werden könnten, wenn KI-Systeme menschenähnliche oder übermenschliche Fähigkeiten erreichen.

### Einfluss und Bedeutung {.explanation}

MIRI prägte maßgeblich den Diskurs zur KI-Sicherheit:

- **Konzeptionelle Grundlagen**: führte zentrale Begriffe wie [AI Alignment](#AI-Alignment) in den Fachdiskurs ein
- **Frühe Warnungen**: thematisierte KI-Risiken lange vor dem Mainstream
- **Talentförderung**: bildete Forscher aus, die später zu führenden Persönlichkeiten im KI-Sicherheitsbereich wurden
- **Gemeinschaftsbildung**: half bei der Etablierung einer Community für langfristige KI-Sicherheitsforschung
- **Einfluss auf die Industrie**: beeinflusste Sicherheitskonzepte bei Organisationen wie [OpenAI](#OpenAI) und [Anthropic](#Anthropic)

Trotz seiner relativ geringen Größe übte MIRI erheblichen Einfluss auf die Entwicklung des KI-Sicherheitsfeldes aus.

### Methodische Ansätze {.explanation}

MIRI vertritt spezifische Forschungsansätze:

- **Formalismus**: nutzt mathematische Präzision zur Beschreibung von KI-Sicherheitsproblemen
- **Deduktiver Ansatz**: versucht, von Grundprinzipien ausgehend Lösungen zu entwickeln
- **Langfristperspektive**: konzentriert sich auf Probleme, die bei fortgeschrittenen KI-Systemen auftreten könnten
- **Vorsorgeprinzip**: betont die Bedeutung präventiver Sicherheitsmaßnahmen
- **Kritische Haltung**: hinterfragt etablierte Annahmen zur KI-Entwicklung

Diese methodischen Präferenzen unterscheiden MIRI von anderen Forschungsgruppen, die empirischere oder praxisnähere Ansätze verfolgen.

### Kontroversen und Kritik {.explanation}

MIRIs Arbeit wurde unterschiedlich aufgenommen:

- **Zeithorizont-Diskussionen**: Kritiker bezweifeln die Relevanz der Forschung für heutige KI-Systeme
- **Methodologische Debatten**: Diskussionen über den Wert formaler versus empirischer Ansätze
- **Priorisierungsfragen**: Kontroversen über die relative Bedeutung existenzieller gegenüber unmittelbaren Risiken
- **Kommunikationsstil**: unterschiedliche Meinungen zur alarmistischen Rhetorik einiger MIRI-Vertreter
- **Akademische Integration**: Kritik an der teilweise begrenzten Einbindung in die traditionelle Wissenschaft

Diese Diskussionen spiegeln breitere Debatten im Feld der KI-Sicherheit wider.

### Verwandte oder andere interessante Themen: {.seealso}

[AI Alignment](#AI-Alignment) |
[AI Risk](#AI-Risk) |
[AI Safety](#AI-Safety) |
[Outer versus Inner Alignment](#Outer-versus-Inner-Alignment) |
[Superintelligence](#Superintelligence) |
[Index](#Index) |

----


