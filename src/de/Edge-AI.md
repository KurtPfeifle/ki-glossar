## Edge AI {#Edge-AI .chapter .small .term}

**Edge AI** bezeichnet die Ausführung von KI-Algorithmen direkt auf lokalen Endgeräten oder Edge-Servern, anstatt in zentralen Cloud-Rechenzentren.

### Kernkonzept {.explanation}

Edge AI verlagert die KI-Verarbeitung näher an die Datenquellen und Endbenutzer.
Diese Dezentralisierung bietet mehrere entscheidende Vorteile gegenüber cloud-basierten KI-Lösungen.

Zu den Hauptmotivationen für Edge AI gehören:

- **Latenzreduzierung**: Schnellere Antwortzeiten durch Eliminierung der Datenübertragung zur Cloud
- **Datenschutz**: Sensible Daten bleiben lokal und müssen nicht übertragen werden
- **Offline-Funktionalität**: KI-Anwendungen funktionieren auch ohne Internetverbindung
- **Bandbreiteneffizienz**: Reduzierung des Datenverkehrs im Netzwerk
- **Energieeffizienz**: Optimierte Verarbeitung auf spezialisierten Edge-Geräten

### Technische Umsetzung {.explanation}

Die Implementierung von Edge AI erfordert spezielle Ansätze zur Optimierung:

- **Modellkomprimierung**: Techniken wie Quantisierung, Pruning und Knowledge Distillation
- **Hardwarebeschleunigung**: Nutzung spezialisierter Chips wie NPUs, TPUs oder FPGAs
- **Spezifische Frameworks**: TensorFlow Lite, ONNX Runtime, PyTorch Mobile
- **Federated Learning**: Verteiltes Training ohne zentralisierte Datenspeicherung

Edge AI findet Anwendung in diversen Bereichen wie:

- Intelligente Kameras und Sicherheitssysteme
- Autonome Fahrzeuge
- Wearables und Gesundheitsgeräte
- Smart-Home-Geräte
- Industrielle IoT-Anwendungen

### Verwandte Themen {.seealso}

[Data Sovereignty](#Data-Sovereignty) |
[DSGVO](#DSGVO) |
[Federated Learning](#Federated-Learning) |
[Hardware Acceleration](#Hardware-Acceleration) |
[Inference](#Inference) |
[Model Compression](#Model-Compression) |
[Quantization](#Quantization) |
[Small Language Models](#Small-Language-Models) |
[Index](#Index) |

----


