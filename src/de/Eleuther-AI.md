## Eleuther AI {#Eleuther-AI .chapter .small .term}

***Kollektiv zur offenen Enwicklung von KI-Technologien ("Open Source")***

**Eleuther AI** ist ein dezentrales Forschungskollektiv, das sich auf die offene Entwicklung und Erforschung großer Sprachmodelle und andere KI-Technologien konzentriert.

### Entstehung und Mission {.explanation}

Eleuther AI wurde im Juli 2020 als Community-getriebene Initiative gegründet.
Das Kollektiv entstand als Reaktion auf die zunehmende Kommerzialisierung und eingeschränkte Verfügbarkeit fortschrittlicher KI-Modelle.

Die Kernziele von Eleuther AI umfassen:

- **Demokratisierung von KI**: Verfügbarmachung fortschrittlicher KI-Technologien für die Allgemeinheit
- **Open Research**: Förderung transparenter und gemeinschaftlicher Forschung
- **Sicherheit und Ethik**: Verantwortungsvolle Entwicklung von KI-Technologien

Der Name "Eleuther" leitet sich vom griechischen Wort für "Freiheit" ab und spiegelt die Philosophie des Kollektivs wider.

### Bedeutende Beiträge {.explanation}

Eleuther AI hat mehrere wichtige Projekte und Ressourcen entwickelt:

- **GPT-NeoX und GPT-J**: Open-Source-Sprachmodelle als Alternative zu kommerziellen Modellen wie GPT-3
- **BLOOM**: Beteiligung am BLOOM-Projekt, einem mehrsprachigen Open-Source-LLM
- **The Pile**: Ein diverser Open-Source-Datensatz für das Training von Sprachmodellen
- **LM Evaluation Harness**: Framework zur standardisierten Bewertung von Sprachmodellen
- **OpenLM Leaderboard**: Transparentes Benchmarking offener Sprachmodelle

Das Kollektiv arbeitet primär über Discord und GitHub und besteht aus Forschern, Ingenieuren und Enthusiasten aus der ganzen Welt.
Trotz begrenzter Ressourcen im Vergleich zu kommerziellen Akteuren hat Eleuther AI bedeutende Fortschritte in der Demokratisierung von KI-Technologie erzielt.

### Verwandte Themen {.seealso}

[Foundation Model](#Foundation-Model) |
[Generative Pre-Trained Transformer](#Generative-Pre-Trained-Transformer) |
[GPT-3](#GPT-3) |
[Large Language Model](#Large-Language-Model) |
[LLM Evaluation](#LLM-Evaluation) |
[Open Pre-trained Transformers](#Open-Pre-trained-Transformers) |
[Training Data](#Training-Data) |
[Index](#Index) |

----


